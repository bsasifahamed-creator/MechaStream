# üöÄ Gu√≠a de Integraci√≥n: Next.js + vLLM API

## ‚úÖ Estado Actual

Tu aplicaci√≥n ahora est√° completamente integrada con vLLM:

- **Next.js App**: Ejecut√°ndose en `http://localhost:3001`
- **vLLM API**: Ejecut√°ndose en `http://localhost:8000`
- **Integraci√≥n**: Completada y funcionando

## üìÅ Archivos Creados

### Backend (API vLLM)
- `vllm_api.py` - Servidor principal de vLLM
- `simple_api.py` - API de prueba (actualmente activa)
- `requirements.txt` - Dependencias Python
- `config.py` - Configuraci√≥n
- `start_server.py` - Script de inicio
- `test_api.py` - Script de pruebas

### Frontend (Next.js)
- `src/lib/vllm-client.ts` - Cliente TypeScript para vLLM
- `src/components/VLLMTest.tsx` - Componente de prueba
- `src/app/test-vllm/page.tsx` - P√°gina de prueba
- `src/app/api/ai/route.ts` - API route actualizada para usar vLLM

## üéØ C√≥mo Usar

### 1. Probar la Integraci√≥n
Visita: `http://localhost:3001/test-vllm`

Esta p√°gina te permite:
- ‚úÖ Verificar el estado de la API
- ‚úÖ Probar generaci√≥n de texto
- ‚úÖ Probar chat completions
- ‚úÖ Ver respuestas en tiempo real

### 2. Usar en tu C√≥digo

```typescript
import { useVLLM } from '@/lib/vllm-client'

function MyComponent() {
  const { generateText, chatCompletion } = useVLLM()
  
  const handleGenerate = async () => {
    const result = await generateText({
      prompt: "Crea un componente React",
      max_tokens: 200,
      temperature: 0.7
    })
    console.log(result.text)
  }
  
  return <button onClick={handleGenerate}>Generar</button>
}
```

### 3. Usar la API Route Actualizada

Tu API route en `/api/ai` ahora usa vLLM:

```javascript
// Desde el frontend
const response = await fetch('/api/ai', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    prompt: 'Crear un bot√≥n React',
    operation: 'generate'
  })
})

const data = await response.json()
console.log(data.content) // C√≥digo generado por vLLM
```

## üîÑ Cambiar de API Simulada a vLLM Real

Actualmente est√°s usando `simple_api.py` (API simulada). Para usar vLLM real:

### 1. Detener API actual
```bash
# En PowerShell
taskkill /f /im python.exe
taskkill /f /im python3.11.exe
```

### 2. Instalar dependencias vLLM
```bash
pip install vllm torch transformers
```

### 3. Ejecutar vLLM real
```bash
python vllm_api.py
# o
python start_server.py --env development
```

**‚ö†Ô∏è Nota**: vLLM real requiere:
- GPU NVIDIA (recomendado)
- 8GB+ RAM
- Primera ejecuci√≥n descarga el modelo (~10-30 min)

## üõ†Ô∏è Configuraci√≥n

### Variables de Entorno (.env.local)
```bash
# Para Next.js
VLLM_BASE_URL=http://localhost:8000

# Para vLLM (opcional)
MODEL_NAME=meta-llama/Meta-Llama-3-8B-Instruct
GPU_MEMORY_UTILIZATION=0.8
PORT=8000
```

## üß™ Endpoints Disponibles

### API vLLM (Puerto 8000)
- `GET /health` - Estado de la API
- `POST /v1/generate` - Generaci√≥n de texto
- `POST /v1/chat/completions` - Chat completions
- `GET /v1/models` - Modelos disponibles

### Next.js API (Puerto 3001)
- `POST /api/ai` - Proxy a vLLM con l√≥gica espec√≠fica

## üîç Debugging

### Verificar Estado
```bash
# PowerShell
Invoke-RestMethod -Uri "http://localhost:8000/health"
```

### Ver Logs
- API vLLM: Logs en consola donde ejecutaste el servidor
- Next.js: Logs en consola del navegador (F12)

### Problemas Comunes

1. **"Connection refused"**
   - Verifica que la API est√© ejecut√°ndose
   - Usa `python test_api.py` para diagnosticar

2. **"Model not loaded"**
   - Espera a que el modelo se descargue/cargue
   - Primera vez puede tardar 10-30 minutos

3. **Error de CORS**
   - Ya est√° configurado para localhost:3000 y 3001
   - Verifica que uses los puertos correctos

## üöÄ Pr√≥ximos Pasos

1. **Integrar en tu App**: Usa el cliente vLLM en tus componentes existentes
2. **Personalizar Prompts**: Modifica los prompts del sistema en `/api/ai/route.ts`
3. **Streaming**: Implementa respuestas en streaming para mejor UX
4. **Caching**: A√±ade cache para respuestas frecuentes
5. **Monitoring**: Implementa m√©tricas y logging

## üìû Soporte

Si tienes problemas:

1. Ejecuta `python test_api.py` para diagnosticar
2. Revisa los logs en consola
3. Verifica que ambos servidores est√©n ejecut√°ndose
4. Visita `http://localhost:3001/test-vllm` para pruebas interactivas

¬°Tu integraci√≥n est√° completa y funcionando! üéâ